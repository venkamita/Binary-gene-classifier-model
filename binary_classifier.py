# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vmVAJTVcoCnsH7yJ66Kdhng182KMIeZX
"""

import pandas as pd
import numpy as np
from scipy import stats
from datasets import load_dataset#stream dataset than download local
ds = load_dataset("macwiatrak/bacbench-essential-genes-dna", split="validation")

print(ds)

ds=ds.remove_columns(['genome_name',  'start', 'end', 'protein_id', 'strand', 'product', '__index_level_0__'])#drop unnecesary columns

print(ds)

ds1=load_dataset("macwiatrak/bacbench-essential-genes-dna", split="train")#load train split
ds1=ds1.remove_columns(['genome_name',  'start', 'end', 'protein_id', 'strand', 'product', '__index_level_0__'])#drop unnecesary columns
print(ds1)

ds2=load_dataset("macwiatrak/bacbench-essential-genes-dna", split="test")#load test split
ds2=ds2.remove_columns(['genome_name',  'start', 'end', 'protein_id', 'strand', 'product', '__index_level_0__'])#drop unnecesary columns
print(ds2)

print(ds1[0])

print(ds1['essential'])

ds = load_dataset("macwiatrak/bacbench-essential-genes-dna", split="validation")

print(ds)

ds=ds.remove_columns(['genome_name',  'start', 'end', 'protein_id', 'strand', 'product', '__index_level_0__'])#drop unnecesary columns
print(ds)

print(len(ds[0]["essential"]))

print(ds[0]["essential"])

def convert(example):
    example["essential"] = [1 if x == "Yes" else 0 for x in example["essential"]]
    return example

ds = ds.map(convert)

def convert(example):
    example["essential"] = [1 if x == "Yes" else 0 for x in example["essential"]]
    return example

ds1= ds1.map(convert)

def convert(example):
    example["essential"] = [1 if x == "Yes" else 0 for x in example["essential"]]
    return example

ds2 = ds2.map(convert)

print(len(ds[0]["dna_seq"]))

# Define mapping
dna_map = {"A": 0, "T": 1, "C": 2, "G": 3,"N":4,"K":5,"R":6,"S":7,"Y":8,"M":9,"W":10}#there is loss of information with this mapping but I wanted to prioritise performance over accuracy

# Encode train split
for i in range(len(ds1)):
    ds1[i]["dna_seq"] = [dna_map[base] for base in ds1[i]["dna_seq"]]

# Encode validation split
for i in range(len(ds)):
    ds[i]["dna_seq"] = [dna_map[base] for base in ds[i]["dna_seq"]]

# Encode test split
for i in range(len(ds2)):
    ds2[i]["dna_seq"] = [dna_map[base] for base in ds2[i]["dna_seq"]]

import numpy as np
from scipy.sparse import csr_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

# --- DNA mapping ---
dna_map = {"A": 0, "T": 1, "C": 2, "G": 3,
           "N":4, "K":5, "R":6, "S":7, "Y":8, "M":9, "W":10}

# --- Parameters ---
NUM_BASES = len(dna_map)          # 11
NUM_4MERS = NUM_BASES ** 4        # 14641 possible 4-mers
STEP = 4                           # non-overlapping 4-mers

# --- Convert string sequence to list of integers using dna_map ---
def encode_sequence(seq, mapping=dna_map):
    return [mapping[base] for base in seq if base in mapping]

# --- Convert sequence to 4-mer counts ---
def sequence_to_4mer_counts(seq, step=STEP):
    counts = np.zeros(NUM_4MERS, dtype=int)
    # The loop condition must ensure there are at least 4 bases for a 4-mer
    for i in range(0, len(seq) - (step - 1), step):
        kmer = seq[i:i+step]
        if len(kmer) < step:
            continue # Should not happen with corrected range, but good for safety

        # kmer is already a list of integers
        # Calculate integer representation of the kmer
        kmer_int = kmer[0]*NUM_BASES**3 + kmer[1]*NUM_BASES**2 + kmer[2]*NUM_BASES + kmer[3]
        counts[kmer_int] += 1
    return counts

# --- Prepare dataset ---
def prepare_dataset(ds_split):
    # Apply the conversion of 'dna_seq' strings to lists of integers using .map()
    # This ensures changes are properly applied to the Dataset object.
    def _map_dna_sequence_to_integers(batch):
        return {"dna_seq": [encode_sequence(seq_str) for seq_str in batch["dna_seq"]]}

    ds_processed = ds_split.map(_map_dna_sequence_to_integers, batched=True)

    # Convert the processed sequences (list of integers) to 4-mer count vectors.
    # sequence_to_4mer_counts now receives lists of integers, as expected.
    X_dense = np.array([sequence_to_4mer_counts(item["dna_seq"]) for item in ds_processed])

    # Extract 'essential' labels. After previous `convert` maps,
    # item["essential"] is a list containing a single integer (e.g., [1]).
    # We need to extract this single integer to form a 1D array for y.
    y = np.array([item["essential"][0] for item in ds_processed])

    return X_dense, y

# --- Convert all splits ---
X_train_dense, y_train = prepare_dataset(ds1)
X_val_dense, y_val = prepare_dataset(ds)
X_test_dense, y_test = prepare_dataset(ds2)

# Convert dense arrays to sparse matrices for memory efficiency if needed
X_train = csr_matrix(X_train_dense)
X_val = csr_matrix(X_val_dense)
X_test = csr_matrix(X_test_dense)

# --- Logistic Regression ---
clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1)
clf.fit(X_train, y_train)

# --- Evaluate ---
y_pred_val = clf.predict(X_val)
print("Validation Accuracy:", accuracy_score(y_val, y_pred_val))
print("Validation F1 Score:", f1_score(y_val, y_pred_val))

y_pred_test = clf.predict(X_test)
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))
print("Test F1 Score:", f1_score(y_test, y_pred_test))
